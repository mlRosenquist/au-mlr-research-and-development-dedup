\begin{table*}[t]
  \centering\sffamily
  \renewcommand{\theadfont}{\normalsize\bfseries}
  \setcellgapes{1ex}\makegapedcells
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirowthead{2}{Model} & \multirowthead{2}{Dataset} & \multirowthead{2}{Dev. bits} & \multicolumn{4}{c|}{\bfseries Metric} & \multicolumn{2}{c|}{\bfseries Time} & \multicolumn{2}{c|}{\bfseries Lossy Compression Rate}                                                                                                       \\
    \cline{4-11}
                             &                            &                              & \textbf{acc.}                         & \textbf{f1}                         & \textbf{rec.}                                         & \textbf{prec.} & \textbf{T(ms)} & \textbf{E(ms)} & \textbf{Rate} & \textbf{Memory Overhead (bytes)} \\
    \hhline{|=|=|=|=|=|=|=|=|=|=|=|}
    Original                 & Synthetic                  &                              & 0.75                                  & 0.85                                & 0.73                                                  & 1              & 78.8           & 51.4           & 1             & 0                                \\

    \clineB{2-11}{2.5}
                             & Pendigits                  &                              & 0.54                                  & 0.70                                & 0.54                                                  & 1              & \textbf{93.7}  & \textbf{55.3}  & 1             & 0                                \\

    \clineB{2-11}{2.5}
                             & WBC                        &                              & 0.94                                  & 0.97                                & 0.94                                                  & 1              & 75.2           & 22.0           & 1             & 0                                \\
    \hhline{|=|=|=|=|=|=|=|=|=|=|=|}

    \multirow{9}{*}{Bases}   & \multirow{3}{*}{Synthetic} & 1                            & 0.72                                  & 0.83                                & 0.71                                                  & 1              & 80.2           & 51.2           & 0.35          & 0                                \\
    \cline{3-11}
                             &                            & 3                            & 0.90                                  & 0.95                                & 0.90                                                  & 1              & 74.6           & 46.4           & 0.08          & 0                                \\
    \cline{3-11}
                             &                            & 6                            & 0.04                                  & 0                                   & 0                                                     & \textbf{0}     & 74.3           & 45.0           & 0.01          & 0                                \\
    \clineB{2-11}{2.5}
                             & \multirow{3}{*}{Pendigits} & 1                            & \textbf{0.53}                         & 0.70                                & 0.53                                                  & 1              & 95.1           & 55.7           & 0.875         & 0                                \\
    \cline{3-11}
                             &                            & 3                            & \textbf{0.51}                         & 0.67                                & 0.51                                                  & 1              & 94.3           & 55.9           & 0.625         & 0                                \\
    \cline{3-11}
                             &                            & 6                            & \textbf{0.42}                         & 0.59                                & 0.41                                                  & 1              & 95.3           & 56.0           & 0.06          & 0                                \\
    \clineB{2-11}{2.5}
                             & \multirow{3}{*}{WBC}       & 1                            & 0.94                                  & 0.97                                & 0.94                                                  & 1              & 75.3           & 21.6           & 0.58          & 0                                \\
    \cline{3-11}
                             &                            & 3                            & 0.94                                  & 0.97                                & 0.94                                                  & 1              & 75.6           & 21.6           & 0.41          & 0                                \\
    \cline{3-11}
                             &                            & 6                            & 0.95                                  & 0.97                                & 0.94                                                  & 1              & 74.8           & 21.4           & 0.02          & 0                                \\
    \hhline{|=|=|=|=|=|=|=|=|=|=|=|}
    \multirow{9}{*}{DupRes}  & \multirow{3}{*}{Synthetic} & 1                            & 0.87                                  & 0.93                                & 0.87                                                  & 1              & 90.4           & 59.7           & 0.35          & 195                              \\
    \cline{3-11}
                             &                            & 3                            & 0.92                                  & 0.95                                & 0.91                                                  & 1              & 85.8           & 55.4           & 0.08          & 56                               \\
    \cline{3-11}
                             &                            & 6                            & 0.98                                  & 0.99                                & 1.00                                                  & \textbf{0.98}  & 86.5           & 53.4           & 0.01          & 6                                \\
    \clineB{2-11}{2.5}
                             & \multirow{3}{*}{Pendigits} & 1                            & \textbf{0.54}                         & 0.70                                & 0.53                                                  & 1              & \textbf{135}   & \textbf{282}   & 0.875         & 118410                           \\
    \cline{3-11}
                             &                            & 3                            & \textbf{0.50}                         & 0.67                                & 0.50                                                  & 1              & 135            & 286            & 0.625         & 86680                            \\
    \cline{3-11}
                             &                            & 6                            & \textbf{0.80}                         & 0.89                                & 0.80                                                  & 1              & \textbf{145}   & \textbf{131}   & 0.06          & 4245                             \\
    \clineB{2-11}{2.5}
                             & \multirow{3}{*}{WBC}       & 1                            & 0.94                                  & 0.97                                & 0.94                                                  & 1              & 86.9           & 23.4           & 0.58          & 5123                             \\
    \cline{3-11}
                             &                            & 3                            & 0.94                                  & 0.97                                & 0.94                                                  & 1              & 87.7           & 23.7           & 0.41          & 3713                             \\
    \cline{3-11}
                             &                            & 6                            & 0.95                                  & 0.97                                & 0.94                                                  & 1              & 85.6           & 23.5           & 0.02          & 153                              \\
    \hline
  \end{tabular}
  \caption{Table containing results of the conducted experiments. The original model performing iForest on uncompressed data. Bases performing iForest on the compressed Data. DupRes which performs extended iForest on the compressed data. Shows the performance metrics(accuracy, f1, recall and precision) and execution time of the various datasets with different amount of compression.}
  \label{tab:comparison}
\end{table*}

This section will present and interpret the results from the conducted experiments. There are many experimental setups. Three different models evaluated on five datasets with varying amount of compression. Then in addition to that, there is many metrics to be looked at. To illustrate this comprehensive amount of results, Table \ref{tab:comparison} has been constructed. The table contains the results of three of the five datasets. The table will be referred to repeatedly in the following examination of the results. The values that are referred to is marked with bold.

\subsection{Execution time}
Starting off with looking at the execution time. The training and test time on the synthetic dataset can visually be seen on Figure \ref{fig:performance_time}. It shows no apparent difference between neither the training or testing time of the original and bases model. However, there is a clear difference between those two and DupRes. Both the training in which the uniques are found and in the testing in which the duplicates are looked up adds additional execution time. This performance cost can also be seen in the table for the pendigits dataset. Here we have 93.7/55.3 ms train/test time for the original model, while DupRes with 1 deviation bit has 135/282 ms train/test time. Additionally, it can be seen that DupRes with 6 deviation bits has 145/131 train/test time. This implies that, as the amount of deviation bits rise, the testing time decreases. This makes sense as less uniques are found during the training phase, and thus less entries in the table need to be looked through during testing. This incentivize to not use the model if a low amount of duplicates is expected in the data or a low amount of deviation bits is utilized. Originally an implementation utilizing pandas'\cite{pandas} dataframes was used. However, this performed notably worse then the current implementation which uses numpy\cite{numpy} arrays. To optimize this further an implementation could be made in c or c++ and utilize the interoperability to Python.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{images/performance_time.png}
  \caption{Training and testing time of the models on the synthetic dataset.}
  \label{fig:performance_time}
\end{figure}

\subsection{Metrics and Scoring}
Then we look at the performance of the models in terms of metrics and scoring. Figure \ref{fig:performance_metrics} illustrates the performance metrics of the models on the datasets. In general it can be seen on the figure that the models are performing quite similar. Investigating the metrics for the synthetic dataset a clear pattern can be seen. The bases model performs better than the original, meanwhile DupRes performs better than the bases. The cause of this can be derived from the recall. Since the positives are the inliers the original is simply classifying more inliers as outliers than the others. It is using too few splits in isolating them. As we compress the data the inliers are harder to isolate due to clustering. Utilizing the many duplicates in DupRes we are then classifying less inliers as outliers. Similar trends is not be seen on the other datasets. On the recall of the pendigits dataset it is seen that the original model predicts less false positives than the others.

Looking at the table there is several interesting observations. All models except two detect every outlier. The two instances are the bases model and DupRes on the synthetic dataset with 6 deviation bits. The bases model predicts every observation as an outlier resulting in 0 scores. Since a lot of data points is grouped with this amount of deviation bits, DupRes will not do the same action as the other model, and therefore have a precision score of 0.98. This implies that one outlier observation might have grouped with the inliers. DupRes will wrongfully push this to be an inlier. Investigating the performance regarding the WBC set it is seen that the performance is almost identical. There are minor discrepancies in the accuracy however the rest is identical. This implies that even though compression is performed, the analytical capabilities is still intact. This is a general trend across all datasets, as was also depicted on Figure \ref{fig:performance_metrics}.

The amount of deviation bits shows a pattern. Looking at the Pendigits dataset the higher the amount of deviation bits the more inliers will be classified as an outlier. This is natural as the compression will place the observations in larger bins that are easier to separate. DupRes does not wrongfully classify these inliers as the large binning has resulted in more duplicates and therefore pushing their score towards being inliers.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{images/performance_metrics.png}
  \caption{Performance metrics of the models with 3 deviation bits. }
  \label{fig:performance_metrics}
\end{figure}
\subsection{Compression and Memory}
The two right-most column in the table describes the compression rate and memory overhead for each model. The compression rate is calculated by dividing the number of compressed bits with the original number of bits. As each feature in every dataset can be represented as a byte, the uncompressed size is: $size_{org} = N \cdot 8 \cdot f $, where $N$ is the amount of samples and $f$ is the amount of features. The lossy compression, without deviation, will then be: $size_{lossy}=N_{unique} \cdot (8-d) \cdot f$, where $N_{unique}$ is the amount of unique samples and $d$ is the amount of deviation bits. The calculation for $size_{lossy}$ is simplified in the sense, that it is neglected that features accross samples can be the same. In the simplified version we only look at the uniqueness accross an entire sample. Therefore, the actual compression rate will be better than despicted in the table. The memory overhead covers the amount of bytes necessary to contain the table of uniques used in DupRes. The calculation is $overhead = (f \cdot (8-d) \cdot N_{unique} + 8 \cdot N_{unique})/8$. This calculation covers storing each unique sample and an eight bits number to store the count. This calculation is simplified in a similar manner to the lossy compression, but aswell under the assumption that the count does not exceed eight bits.          

Looking at the table the original model naturally has no compression. The two others compression varies on the their amount of deviation bits. $d=6$ for both of the models catches all outliers except on the synthetic data set as described previously. This is a quite significant compression rate of 0.06 on Pendigits and 0.02 on WBC.  The memory overhead shows the trade off with DupRes and performing original Isolation Forest on the bases. They have the same compression rate. However, DupRes needs additional memory to store the table. The trend between amount of deviation bits and memory overhead is clearly seen in the table. This emphasize the fact that DupRes is penalized when there is a low amount of duplicates. The overhead combined with the time spent creating the table and the testing time looking up in the table is not to be neglected.  


The expansive growth of Internet of Things (IoT) enables many revolutionary and futuristic applications. However, the large quantity of sensors leads to an immense volume of data needing to be processed at the edge and potentially forwarded to the cloud. Hence, the topology meets severe challenges regarding data transmission and storage. Compression is the apparent mechanism to address these challenges by reducing the volume being transmitted and stored. Compressing the data does not come without disadvantages. Processing time is required during compression and decompression. Additionally, it might not be possible to perform various analytics on the compressed data, as it might have lost its intrinsic meaning. This leads to a trade-off between compression and distortion\cite{anomaly-det-compressed}.  The possibility of performing analytics varies on the compression algorithm and whether it is lossy or lossless. Being able to perform analytics on the compressed data enables edge nodes to act more intelligently without the drawbacks of decompression. Analytics of interest are techniques such as clustering, classification, basic queries (e.g. mean and variance) and outlier detection. Depending on the results of an analytic the node could make intermediate decisions and improve the performance and overall functionality of the system.      

Performing analytics on compressed data is not an untouched subject. There are researchers looking at different aspects, analytics and compression algorithms. Work has been done in performing classification and anomaly detection within network communication on compressed data \cite{related-1}. It is done by developing a novel collection of algorithms and models that uses compression techniques to identify classes and anomalies. The core idea of the method is to utilize their new slice compression (SC) as a pre-processing step to identify parts of data as either application or protocol data. This is used for classification and extended to slice compression for anomaly detection (SCADe). SCADe utilizes the slice compression in combination with a threshold for identifying anomalous data. Slice compression is lossy and used to tell what features are need to be examined. Therefore, there is no need to analyze all decompressed data, but instead only analyze the features depicted by slice compression.

Lossy compression can by definition obtain better compression than lossless compression, and still maintain key information regarding the performance of analytics. However, the data distortion is not feasible for all applications. Being able to easily decide between lossy and lossless compression at different parts of the system is therefore a desired quality. We will use Generalized Deduplication\cite{gen-deduplication} (GD) which is a lossless compression technique, however it does not require much change to become lossy. GD has desired features such as easy explainability, suits IoT data and good random access. The efficient random access is key and enables the analytical capabilities. Previous work has been done regarding analytics of GD compressed data\cite{dir-analytics-gd}. It researches how clustering can be performed on an artificial, an artificial with nose and a power consumption data set. The research showed that even with high compression rate it was still performing close to the uncompressed data sets.               

This paper will look at performing anomaly detection on GD compressed data. There are many anomaly detection methods suited for IoT data\cite{survey_outlier}. Each have their own advantages and disadvantages. Some are supervised, distributed, online while others are unsupervised, centralized and offline. We will utilize the ensemble tree method Isolation Forest (iForest). The reasoning is that it is simple, well-suited for IoT data and normalization of features is not required. Isolation Forest works by performing horizontal and vertical splits in the feature space, and thereby identifying the anomalies by seeing how easy they are to isolate. The horizontal and vertical splits is a potential flaw and can result in certain anomalies not being detected. This is handled in an extended version, that performs diagonal splits \cite{extended-iforest}. We will not utilize the diagonal splits, however we propose our own extension that is tailored for GD compressed data. GD natively creates a large amount of duplicates. Therefore, we look into utilizing the amount of duplicates to avoid wrongly classifying inliers as outliers. 

The rest of this paper covers the background of used methods in Section \ref{sec:background}, the proposed methods in Section \ref{sec:methods}, the conducted experiments in Section \ref{sec:experiments} and an evaluation of the results in Section \ref{sec:results}. The paper is concluded in Section \ref{sec:conclusion}.